# Odly üèùÔ∏è

**Personal Knowledge Management with On-Device AI**

A React Native application that combines on-device LLM inference with personal knowledge management. Built for privacy-first AI interaction, Odly runs entirely offline using the TinyLlama GGUF model to help you capture thoughts, search knowledge, and manage markdown files.

**üìñ For product features and usage guide, see [FEATURES.md](./FEATURES.md)**

## üåü Overview

Odly is a mobile-first knowledge management system that uses AI for:
- On-device LLM inference with TinyLlama
- Natural language search across markdown files
- Conversational chat interface with tagging
- AI-powered message grouping
- Privacy-first, offline-only operation

**Key Differentiator:** All AI processing happens on-device. No internet required, no data leaves your phone.

---

## üöÄ Quick Start

**Development Mode** (with Metro bundler):
```bash
npm run warmup
```
Installs dependencies, starts Metro, launches app on connected device.

**Deploy Mode** (standalone debug APK):
```bash
npm run deploy
```
Bundles JS and builds debug APK. Works without Metro running. Allows file sync with `pull-files.sh`.

---

## ‚ú® Features

**Three-Tab Interface:** Chat üí≠ | Search üßû | Files üè∞

- **100% Offline**: No internet required for AI inference
- **Privacy-First**: All data stays on your device
- **Fast Inference**: Optimized TinyLlama model for mobile
- **Smart Organization**: AI-powered message grouping and tagging
- **Source Attribution**: See which files contributed to answers
- **File Management**: Direct markdown editing in-app

**üëâ For detailed features and usage guide, see [FEATURES.md](./FEATURES.md)**

---

## üèóÔ∏è Architecture

### Project Structure

```
src/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ LLMQueryApp.tsx           # Main app container with tab navigation
‚îÇ   ‚îú‚îÄ‚îÄ ChatInterface.tsx         # Chat/notes interface
‚îÇ   ‚îú‚îÄ‚îÄ QueryInterface.tsx        # Search interface
‚îÇ   ‚îî‚îÄ‚îÄ FileExplorerInterface.tsx # File browser and editor
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ LLMService.ts             # TinyLlama GGUF inference engine
‚îÇ   ‚îú‚îÄ‚îÄ FileService.ts            # File I/O operations
‚îÇ   ‚îú‚îÄ‚îÄ AnswerService.ts          # Search query orchestration
‚îÇ   ‚îú‚îÄ‚îÄ ChatService.ts            # Chat message persistence
‚îÇ   ‚îî‚îÄ‚îÄ GroupbyService.ts         # AI-powered message grouping
‚îî‚îÄ‚îÄ index.ts

android/app/src/main/assets/
‚îú‚îÄ‚îÄ aham/                         # Knowledge base directory
‚îÇ   ‚îú‚îÄ‚îÄ gig.md                    # Computer history & tech
‚îÇ   ‚îú‚îÄ‚îÄ fun.md                    # Fun & activities
‚îÇ   ‚îú‚îÄ‚îÄ love.md                   # Love & connection
‚îÇ   ‚îú‚îÄ‚îÄ play.md                   # Play & games
‚îÇ   ‚îî‚îÄ‚îÄ work.md                   # Work & productivity
‚îî‚îÄ‚îÄ tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf  # TinyLlama model
```

### Service Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         LLMQueryApp (Main Container)        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Chat    ‚îÇ    Search    ‚îÇ    Files    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ     üí≠    ‚îÇ      üßû      ‚îÇ     üè∞      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ         ‚îÇ            ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ                ‚îÇ                   ‚îÇ
       ‚ñº                ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇChatService  ‚îÇ  ‚îÇAnswerService ‚îÇ   ‚îÇFileService ‚îÇ
‚îÇ   - Save    ‚îÇ  ‚îÇ  - Query     ‚îÇ   ‚îÇ  - Read    ‚îÇ
‚îÇ   - Group   ‚îÇ  ‚îÇ  - Search    ‚îÇ   ‚îÇ  - Write   ‚îÇ
‚îÇ   - Push    ‚îÇ  ‚îÇ  - Sources   ‚îÇ   ‚îÇ  - List    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                ‚îÇ                   ‚îÇ
       ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
       ‚îÇ           ‚îÇ          ‚îÇ            ‚îÇ
       ‚ñº           ‚ñº          ‚ñº            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           LLMService (TinyLlama)           ‚îÇ
‚îÇ         - On-device inference              ‚îÇ
‚îÇ         - Context-aware generation         ‚îÇ
‚îÇ         - 2048 token context window        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ llama.rn (llama.cpp)  ‚îÇ
        ‚îÇ  GGUF Model Runtime   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Tech Stack

- **React Native 0.74.1**: Bare React Native for maximum control
- **llama.rn 0.7.0**: React Native bindings for llama.cpp GGUF inference
- **react-native-fs 2.20.0**: Filesystem access for assets and document storage
- **TypeScript 5.0.4**: Type safety and developer experience
- **TinyLlama 1.1B**: Compact, fast LLM optimized for mobile devices

---

## üì¶ Setup Instructions

### Prerequisites

- **Node.js** >= 18
- **Android Studio** with Android SDK configured
- **Connected Android device or emulator**

### Installation

1. **Clone and install dependencies:**
   ```bash
   cd /Users/abhinitmodi/Dev/odly
   npm install
   ```

2. **Android Setup:**
   - Ensure Android SDK is properly configured
   - Connect an Android device or start an emulator

### Running the App

```bash
npm run android
```

**Start Metro bundler separately:**
```bash
npm start
```

### Building Release Versions

The app can run without the Metro bundler by creating release bundles:

**Create JavaScript bundle:**
```bash
npm run bundle:android
```

**Build release APK (Android):**
```bash
# Standard APK
npm run build:android:release

# Android App Bundle (AAB) for Play Store
npm run build:android:release-aab
```

The release APK will be located at:
```
android/app/build/outputs/apk/release/app-release.apk
```

## üîß Model Configuration

### Current Model

**TinyLlama 1.1B Chat (Q4_K_M)**
- **Size**: ~700MB
- **Format**: GGUF (4-bit quantized)
- **Context**: 2048 tokens
- **Speed**: ~2-3 paragraphs in 15-20 seconds
- **Memory**: Optimized for mobile devices

### LLM Settings

Current configuration in `LLMService.ts`:
```typescript
{
  model: this.modelPath,
  use_mlock: false,        // Reduce memory pressure
  n_ctx: 2048,             // Context window
  n_batch: 256,            // Batch size
  n_threads: 2,            // Thread count
  n_gpu_layers: 0,         // CPU only
  n_predict: 512,          // Response length
  temperature: 0.7,        // Creativity
  top_p: 0.9,              // Nucleus sampling
  repeat_penalty: 1.1,     // Prevent repetition
}
```

### Using Different Models

To use a different GGUF model:

1. **Download a GGUF model** from Hugging Face
2. **Place in** `android/app/src/main/assets/`
3. **Update reference** in initialization code
4. **Recommended mobile models:**
   - TinyLlama 1.1B (current) - Fast, general purpose
   - Phi-3-mini ~2.3GB - Better quality, slower
   - Qwen2-0.5B - Faster, lighter
   - Gemma-2B - Balanced performance

---

## üß™ Development

### Scripts

```bash
npm start           # Start Metro bundler
npm run android     # Run on Android device/emulator
npm run lint        # Run ESLint
npm test            # Run Jest tests
npm run warmup      # Complete setup and launch
npm run deploy      # Install a debug build
```

---

## üêõ Debugging

### View Logs

```bash
# Android
npx react-native log-android

# Android device logs
adb logcat
```

---

## üìÑ License

This project is open source. Please check the model licenses for any GGUF models you use.

**TinyLlama Model**: Apache 2.0 License

---

## üôè Acknowledgments

- **llama.cpp** - Fast inference engine
- **llama.rn** - React Native bindings
- **TinyLlama** - Compact, capable model
- **React Native community** - Excellent tooling

---

**Built with ‚ù§Ô∏è for privacy-first AI interaction**

*Odly - Your personal knowledge companion, powered by on-device intelligence*
